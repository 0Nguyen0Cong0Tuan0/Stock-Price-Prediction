{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "11f5cde8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from pmdarima import auto_arima\n",
    "from prophet import Prophet\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import timedelta\n",
    "import os\n",
    "import logging\n",
    "import cmdstanpy\n",
    "import re\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "# Suppress cmdstanpy debug logs to reduce console clutter\n",
    "cmdstanpy.utils.get_logger().setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "32216b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_prepare_data(stock_csv, balance_csv, cashflow_csv, income_csv, ticker, train_end_date=None):\n",
    "    \"\"\"\n",
    "    Load and preprocess stock and financial data, aligning them for modeling.\n",
    "    Adjusted: Interpolate financial metrics, fit scaler on training data, improve error handling.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load stock data\n",
    "        stock_df = pd.read_csv(stock_csv)\n",
    "        stock_df['Date'] = pd.to_datetime(stock_df['Date'])\n",
    "        stock_df = stock_df[stock_df['Ticker'] == ticker][['Date', 'Close', 'Volume']].sort_values('Date')\n",
    "        stock_df = stock_df.drop_duplicates(subset=['Date', 'Ticker'])\n",
    "        \n",
    "        # Validate stock data\n",
    "        if stock_df['Close'].min() <= 0:\n",
    "            raise ValueError(\"Invalid stock data: Close prices contain zero or negative values\")\n",
    "        \n",
    "        # Create daily date range and fill missing dates\n",
    "        date_range = pd.date_range(start=stock_df['Date'].min(), end=stock_df['Date'].max(), freq='D')\n",
    "        stock_df = stock_df.set_index('Date').reindex(date_range, method='ffill').reset_index()\n",
    "        stock_df = stock_df.rename(columns={'index': 'Date'})\n",
    "        stock_df['Ticker'] = ticker\n",
    "        \n",
    "        logging.info(f\"Loaded {len(stock_df)} daily stock data points for {ticker}\")\n",
    "        \n",
    "        # Load financial data\n",
    "        balance_df = pd.read_csv(balance_csv)\n",
    "        cashflow_df = pd.read_csv(cashflow_csv)\n",
    "        income_df = pd.read_csv(income_csv)\n",
    "        \n",
    "        # Select key financial metrics with quarterly updates\n",
    "        financial_metrics = {\n",
    "            'Diluted EPS': income_df[income_df['index'] == 'Diluted EPS'][['2024-09-30 00:00:00']].iloc[0, 0],\n",
    "            'Free Cash Flow': cashflow_df[cashflow_df['index'] == 'Free Cash Flow'][['2024-09-30 00:00:00']].iloc[0, 0],\n",
    "            'Net Debt': balance_df[balance_df['index'] == 'Net Debt'][['2024-09-30 00:00:00']].iloc[0, 0],\n",
    "            'EBITDA': income_df[income_df['index'] == 'EBITDA'][['2024-09-30 00:00:00']].iloc[0, 0]\n",
    "        }\n",
    "        \n",
    "        # Create financial DataFrame with quarterly updates\n",
    "        financial_df = pd.DataFrame(index=date_range)\n",
    "        for metric, value in financial_metrics.items():\n",
    "            # Simulate quarterly updates (e.g., every 90 days)\n",
    "            financial_df[metric] = np.nan\n",
    "            reporting_dates = pd.date_range(start=date_range[0], end=date_range[-1], freq='Q')\n",
    "            for report_date in reporting_dates:\n",
    "                if report_date in financial_df.index:\n",
    "                    financial_df.loc[report_date, metric] = value\n",
    "            financial_df[metric] = financial_df[metric].ffill()\n",
    "        \n",
    "        # Merge stock and financial data\n",
    "        merged_df = stock_df.merge(financial_df.reset_index(), on='Date', how='left')\n",
    "        merged_df = merged_df.set_index('Date')\n",
    "        \n",
    "        # Scale financial features on training data only\n",
    "        scaler = StandardScaler()\n",
    "        financial_cols = ['Diluted EPS', 'Free Cash Flow', 'Net Debt', 'EBITDA']\n",
    "        if train_end_date:\n",
    "            train_data = merged_df[merged_df.index <= train_end_date]\n",
    "            merged_df[financial_cols] = scaler.fit_transform(train_data[financial_cols])\n",
    "        else:\n",
    "            merged_df[financial_cols] = scaler.fit_transform(merged_df[financial_cols])\n",
    "        \n",
    "        logging.info(f\"Prepared data with {len(merged_df)} rows, including financial features: {list(financial_metrics.keys())}\")\n",
    "        return merged_df, scaler\n",
    "    \n",
    "    except FileNotFoundError as e:\n",
    "        logging.error(f\"File not found: {e}. Skipping ticker {ticker}.\")\n",
    "        return None, None\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error preparing data for {ticker}: {e}. Skipping ticker.\")\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b57f0ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Clean text by removing URLs and special characters.\n",
    "    Added: Text preprocessing for sentiment analysis.\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = re.sub(r'http\\S+', '', text)  # Remove URLs\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)  # Remove special characters\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "08bcf515",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_news_data(news_csv, ticker, stock_dates):\n",
    "    \"\"\"\n",
    "    Load and preprocess news data, compute sentiment scores, and align with stock data dates.\n",
    "    Adjusted: Neutral fallback sentiment, text cleaning, performance optimization, date validation.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if not os.path.exists(news_csv):\n",
    "            logging.error(f\"News file not found: {news_csv}\")\n",
    "            raise FileNotFoundError(f\"News file not found: {news_csv}\")\n",
    "        \n",
    "        # Load news data\n",
    "        news_df = pd.read_csv(news_csv)\n",
    "        logging.info(f\"Raw news data shape: {news_df.shape}, columns: {list(news_df.columns)}\")\n",
    "        \n",
    "        # Parse dates\n",
    "        news_df['Date'] = pd.to_datetime(news_df['Date'], errors='coerce').dt.date\n",
    "        invalid_dates = news_df['Date'].isna().sum()\n",
    "        if invalid_dates > 0:\n",
    "            logging.warning(f\"Dropped {invalid_dates} rows due to invalid dates\")\n",
    "        news_df = news_df.dropna(subset=['Date'])\n",
    "        logging.info(f\"After date parsing, news data shape: {news_df.shape}\")\n",
    "        \n",
    "        # Filter for ticker\n",
    "        news_df['Ticker'] = news_df['Ticker'].str.strip().str.upper()\n",
    "        ticker = ticker.strip().upper()\n",
    "        news_df = news_df[news_df['Ticker'] == ticker]\n",
    "        logging.info(f\"After filtering for ticker '{ticker}', news data shape: {news_df.shape}\")\n",
    "        \n",
    "        if news_df.empty:\n",
    "            logging.warning(f\"No news articles found for {ticker}. Using neutral sentiment scores.\")\n",
    "            return pd.DataFrame({'Date': stock_dates, 'Sentiment_Score': 0.0})\n",
    "        \n",
    "        # Initialize sentiment analyzer\n",
    "        analyzer = SentimentIntensityAnalyzer()\n",
    "        \n",
    "        # Compute sentiment with progress bar\n",
    "        def get_sentiment(row):\n",
    "            text = (clean_text(row['Content']) if pd.notna(row['Content']) else\n",
    "                    clean_text(row['Description']) if pd.notna(row['Description']) else\n",
    "                    clean_text(row['Title']) if pd.notna(row['Title']) else \"\")\n",
    "            if not text:\n",
    "                logging.debug(f\"Empty text for row: {row.name}. Returning 0.0\")\n",
    "                return 0.0\n",
    "            scores = analyzer.polarity_scores(text)\n",
    "            return scores['compound']\n",
    "        \n",
    "        news_df['Sentiment_Score'] = [get_sentiment(row) for row in tqdm(news_df.to_dict('records'), desc=\"Sentiment Analysis\")]\n",
    "        logging.info(f\"Sentiment score summary: {news_df['Sentiment_Score'].describe().to_dict()}\")\n",
    "        \n",
    "        # Aggregate sentiment by date\n",
    "        sentiment_df = news_df.groupby('Date')['Sentiment_Score'].mean().reset_index()\n",
    "        sentiment_df['Date'] = pd.to_datetime(sentiment_df['Date'])\n",
    "        logging.info(f\"Aggregated sentiment data shape: {sentiment_df.shape}\")\n",
    "        \n",
    "        # Create full sentiment DataFrame\n",
    "        sentiment_full = pd.DataFrame({'Date': stock_dates, 'Sentiment_Score': 0.0})\n",
    "        sentiment_full = sentiment_full.merge(sentiment_df, on='Date', how='left', suffixes=('', '_news'))\n",
    "        sentiment_full['Sentiment_Score'] = sentiment_full['Sentiment_Score_news'].fillna(0.0)\n",
    "        \n",
    "        # Forward-fill sentiment after first non-zero score\n",
    "        first_news_date = sentiment_full[sentiment_full['Sentiment_Score'] != 0.0]['Date'].min()\n",
    "        if pd.notna(first_news_date):\n",
    "            mask = sentiment_full['Date'] >= first_news_date\n",
    "            sentiment_full.loc[mask, 'Sentiment_Score'] = sentiment_full.loc[mask, 'Sentiment_Score'].ffill()\n",
    "        \n",
    "        sentiment_full = sentiment_full[['Date', 'Sentiment_Score']]\n",
    "        logging.info(f\"Final sentiment data shape: {sentiment_full.shape}, non-zero scores: {sentiment_full['Sentiment_Score'].ne(0).sum()}\")\n",
    "        return sentiment_full\n",
    "    \n",
    "    except FileNotFoundError as e:\n",
    "        logging.error(f\"File not found: {e}\")\n",
    "        return pd.DataFrame({'Date': stock_dates, 'Sentiment_Score': 0.0})\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error processing news data: {e}\")\n",
    "        return pd.DataFrame({'Date': stock_dates, 'Sentiment_Score': 0.0})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "118dbcec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(actual, predicted):\n",
    "    \"\"\"\n",
    "    Calculate RMSE, MAE, and MAPE for model evaluation.\n",
    "    Adjusted: Log filtered data points.\n",
    "    \"\"\"\n",
    "    actual = np.array(actual)\n",
    "    predicted = np.array(predicted)\n",
    "    mask = (actual > 0) & (~np.isnan(actual)) & (~np.isnan(predicted))\n",
    "    actual = actual[mask]\n",
    "    predicted = predicted[mask]\n",
    "    \n",
    "    if len(actual) < len(np.array(actual, copy=True)):\n",
    "        logging.info(f\"Filtered {len(np.array(actual, copy=True)) - len(actual)} invalid data points for metrics calculation\")\n",
    "    \n",
    "    if len(actual) == 0:\n",
    "        logging.warning(\"No valid data for metrics calculation\")\n",
    "        return {'RMSE': np.nan, 'MAE': np.nan, 'MAPE': np.nan}\n",
    "    \n",
    "    rmse = np.sqrt(mean_squared_error(actual, predicted))\n",
    "    mae = mean_absolute_error(actual, predicted)\n",
    "    mape = np.mean(np.abs((actual - predicted) / actual)) * 100 if np.all(actual != 0) else np.nan\n",
    "    return {'RMSE': rmse, 'MAE': mae, 'MAPE': mape}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a6385c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def arima_forecast(data, forecast_horizon=7):\n",
    "    \"\"\"\n",
    "    Fit ARIMA model with stationarity check.\n",
    "    Adjusted: Validate data, handle insufficient data, robust error handling.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Validate data\n",
    "        data = data.dropna()\n",
    "        if data.isna().any() or np.isinf(data).any():\n",
    "            logging.error(\"Invalid data: Contains NaN or inf values\")\n",
    "            return None, None, None\n",
    "        if len(data) < 30:\n",
    "            logging.error(\"Insufficient data for ARIMA (<30 days)\")\n",
    "            return None, None, None\n",
    "        \n",
    "        # Check stationarity\n",
    "        result = adfuller(data)\n",
    "        if result[1] > 0.05:\n",
    "            logging.info(\"Data is non-stationary, applying differencing\")\n",
    "            data_diff = data.diff().dropna()\n",
    "            if len(data_diff) < 10:\n",
    "                logging.error(\"Insufficient data after differencing\")\n",
    "                return None, None, None\n",
    "            model = auto_arima(data_diff, seasonal=False, max_p=7, max_q=7, max_d=2,\n",
    "                              stepwise=True, trace=True, error_action='ignore')\n",
    "            best_order = model.order\n",
    "            arima_model = ARIMA(data_diff, order=best_order).fit()\n",
    "            forecast_diff = arima_model.forecast(steps=forecast_horizon)\n",
    "            forecast = data.iloc[-1] + forecast_diff.cumsum()\n",
    "        else:\n",
    "            model = auto_arima(data, seasonal=False, max_p=7, max_q=7, max_d=2,\n",
    "                              stepwise=True, trace=True, error_action='ignore')\n",
    "            best_order = model.order\n",
    "            arima_model = ARIMA(data, order=best_order).fit()\n",
    "            forecast = arima_model.forecast(steps=forecast_horizon)\n",
    "        \n",
    "        # Evaluate\n",
    "        if len(data) >= forecast_horizon:\n",
    "            test_data = data[-forecast_horizon:]\n",
    "            forecast_test = arima_model.forecast(steps=forecast_horizon)[-forecast_horizon:]\n",
    "            metrics = calculate_metrics(test_data, forecast_test)\n",
    "        else:\n",
    "            metrics = {'RMSE': np.nan, 'MAE': np.nan, 'MAPE': np.nan}\n",
    "        \n",
    "        logging.info(f\"ARIMA Metrics: {metrics}\")\n",
    "        return forecast, metrics, best_order\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error in ARIMA forecasting: {e}\")\n",
    "        return None, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "73566462",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prophet_forecast(data, forecast_horizon=7, changepoint_prior_scale=0.05):\n",
    "    \"\"\"\n",
    "    Fit Prophet model with dynamic regressors.\n",
    "    Adjusted: Forecast regressors, handle duplicates, enable MCMC for small datasets.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        logging.info(f\"Prophet input DataFrame columns: {list(data.columns)}\")\n",
    "        prophet_df = data.reset_index()[['Date', 'Close', 'Diluted EPS', 'Free Cash Flow', 'Net Debt', 'EBITDA', 'Sentiment_Score']]\n",
    "        prophet_df = prophet_df.drop_duplicates(subset='Date')\n",
    "        prophet_df = prophet_df.rename(columns={'Date': 'ds', 'Close': 'y'})\n",
    "        \n",
    "        logging.info(f\"Prophet input data summary:\\n{prophet_df.describe()}\")\n",
    "        \n",
    "        # Fit Prophet model\n",
    "        model = Prophet(daily_seasonality=True, yearly_seasonality=True, weekly_seasonality=True,\n",
    "                       changepoint_prior_scale=changepoint_prior_scale, mcmc_samples=300 if len(prophet_df) < 1000 else 0)\n",
    "        for regressor in ['Diluted EPS', 'Free Cash Flow', 'Net Debt', 'EBITDA', 'Sentiment_Score']:\n",
    "            model.add_regressor(regressor)\n",
    "        model.fit(prophet_df)\n",
    "        \n",
    "        # Create future dataframe with dynamic regressors\n",
    "        future = model.make_future_dataframe(periods=forecast_horizon)\n",
    "        for regressor in ['Diluted EPS', 'Free Cash Flow', 'Net Debt', 'EBITDA']:\n",
    "            future[regressor] = prophet_df[regressor].mean()  # Use mean of historical data\n",
    "        future['Sentiment_Score'] = prophet_df['Sentiment_Score'].iloc[-10:].mean()  # Recent sentiment trend\n",
    "        forecast_df = model.predict(future)\n",
    "        \n",
    "        # Extract forecast\n",
    "        forecast = forecast_df[['ds', 'yhat']].tail(forecast_horizon).set_index('ds')['yhat']\n",
    "        \n",
    "        # Evaluate\n",
    "        test_data = prophet_df['y'][-forecast_horizon:]\n",
    "        forecast_test = model.predict(prophet_df[-forecast_horizon:])['yhat']\n",
    "        metrics = calculate_metrics(test_data, forecast_test)\n",
    "        \n",
    "        logging.info(f\"Prophet Metrics (changepoint_prior_scale={changepoint_prior_scale}): {metrics}\")\n",
    "        return forecast, metrics, forecast_df\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error in Prophet forecasting: {e}\")\n",
    "        return None, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4f50252f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_prophet(data, forecast_horizon=7):\n",
    "    \"\"\"\n",
    "    Tune Prophet model with expanded scale range.\n",
    "    Adjusted: Wider scale range, add cross-validation.\n",
    "    \"\"\"\n",
    "    from prophet.diagnostics import cross_validation\n",
    "    scales = [0.01, 0.05, 0.1, 0.25, 0.5, 1.0]\n",
    "    best_metrics = {'RMSE': float('inf')}\n",
    "    best_forecast = None\n",
    "    best_forecast_df = None\n",
    "    best_scale = 0.05\n",
    "    \n",
    "    for scale in scales:\n",
    "        forecast, metrics, forecast_df = prophet_forecast(data, forecast_horizon, changepoint_prior_scale=scale)\n",
    "        if forecast is not None and not np.any(np.isnan(forecast)):\n",
    "            # Cross-validation\n",
    "            prophet_df = data.reset_index()[['Date', 'Close', 'Diluted EPS', 'Free Cash Flow', 'Net Debt', 'EBITDA', 'Sentiment_Score']]\n",
    "            prophet_df = prophet_df.rename(columns={'Date': 'ds', 'Close': 'y'})\n",
    "            model = Prophet(changepoint_prior_scale=scale)\n",
    "            for regressor in ['Diluted EPS', 'Free Cash Flow', 'Net Debt', 'EBITDA', 'Sentiment_Score']:\n",
    "                model.add_regressor(regressor)\n",
    "            model.fit(prophet_df)\n",
    "            cv_df = cross_validation(model, horizon=f'{forecast_horizon} days', parallel=\"processes\")\n",
    "            rmse = np.sqrt(mean_squared_error(cv_df['y'], cv_df['yhat']))\n",
    "            if rmse < best_metrics['RMSE']:\n",
    "                best_metrics = {'RMSE': rmse}\n",
    "                best_forecast = forecast\n",
    "                best_forecast_df = forecast_df\n",
    "                best_scale = scale\n",
    "    \n",
    "    if best_forecast is None:\n",
    "        logging.warning(\"All Prophet models failed. Using default scale=0.05.\")\n",
    "        forecast, metrics, forecast_df = prophet_forecast(data, forecast_horizon, changepoint_prior_scale=0.05)\n",
    "        best_metrics = metrics if metrics else {'RMSE': np.nan, 'MAE': np.nan, 'MAPE': np.nan}\n",
    "        best_forecast = forecast\n",
    "        best_forecast_df = forecast_df\n",
    "        best_scale = 0.05\n",
    "    \n",
    "    logging.info(f\"Best Prophet changepoint_prior_scale: {best_scale}\")\n",
    "    logging.info(f\"Best Prophet Metrics: {best_metrics}\")\n",
    "    return best_forecast, best_metrics, best_forecast_df, best_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5f710387",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backtest_strategy(data, predictions, model_name, threshold=None):\n",
    "    \"\"\"\n",
    "    Backtest a trading strategy with volatility-based threshold.\n",
    "    Adjusted: Volatility-based threshold, prevent lookahead bias, add transaction costs.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        logging.info(f\"Backtest input lengths - Data: {len(data)}, Predictions: {len(predictions)}\")\n",
    "        if len(predictions) != len(data):\n",
    "            logging.warning(f\"Predictions length ({len(predictions)}) does not match data length ({len(data)}). Truncating.\")\n",
    "            min_length = min(len(data), len(predictions))\n",
    "            data = data.iloc[:min_length]\n",
    "            predictions = predictions[:min_length]\n",
    "        \n",
    "        pred_df = pd.DataFrame({'Date': data.index, 'Close': data['Close'], 'Prediction': predictions})\n",
    "        pred_df = pred_df.dropna()\n",
    "        \n",
    "        # Volatility-based threshold\n",
    "        volatility = pred_df['Close'].pct_change().std()\n",
    "        threshold = volatility * 2 if threshold is None else threshold\n",
    "        \n",
    "        # Generate signals without lookahead bias\n",
    "        pred_df['Signal'] = 0\n",
    "        pred_df.loc[1:, 'Signal'] = np.where(\n",
    "            pred_df['Prediction'][1:] > pred_df['Close'][:-1] * (1 + threshold), 1,\n",
    "            np.where(pred_df['Prediction'][1:] < pred_df['Close'][:-1] * (1 - threshold), -1, 0)\n",
    "        )\n",
    "        \n",
    "        # Calculate returns with transaction costs\n",
    "        pred_df['Return'] = pred_df['Close'].pct_change()\n",
    "        transaction_cost = 0.001  # 0.1% per trade\n",
    "        pred_df['Strategy_Return'] = pred_df['Signal'].shift(1) * pred_df['Return'] - \\\n",
    "                                    pred_df['Signal'].abs().shift(1) * transaction_cost\n",
    "        \n",
    "        # Cumulative return\n",
    "        cumulative_return = (1 + pred_df['Strategy_Return'].dropna()).cumprod().iloc[-1] - 1\n",
    "        num_trades = pred_df['Signal'].abs().sum()\n",
    "        \n",
    "        results = {\n",
    "            'Cumulative Return (%)': cumulative_return * 100,\n",
    "            'Number of Trades': num_trades\n",
    "        }\n",
    "        \n",
    "        logging.info(f\"Backtest Results for {model_name}: {results}\")\n",
    "        return results, pred_df\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error in backtesting: {e}\")\n",
    "        return None, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d4ceff72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def walk_forward_validation(data, forecast_horizon=7, n_folds=10):\n",
    "    \"\"\"\n",
    "    Perform walk-forward validation with robust averaging.\n",
    "    Adjusted: Increase folds, handle missing regressors, robust averaging.\n",
    "    \"\"\"\n",
    "    from scipy.stats import trim_mean\n",
    "    n_folds = min(n_folds, len(data) // forecast_horizon)\n",
    "    arima_metrics_list = []\n",
    "    prophet_metrics_list = []\n",
    "    \n",
    "    for i in range(n_folds):\n",
    "        train_end = len(data) - (n_folds - i) * forecast_horizon\n",
    "        if train_end <= forecast_horizon:\n",
    "            continue\n",
    "        \n",
    "        train_data = data.iloc[:train_end]\n",
    "        test_data = data.iloc[train_end:train_end + forecast_horizon]['Close']\n",
    "        \n",
    "        if len(test_data) != forecast_horizon:\n",
    "            continue\n",
    "        \n",
    "        # ARIMA\n",
    "        arima_model = auto_arima(train_data['Close'], seasonal=False, max_p=7, max_q=7, max_d=2,\n",
    "                                stepwise=True, error_action='ignore')\n",
    "        arima_fit = ARIMA(train_data['Close'], order=arima_model.order).fit()\n",
    "        arima_pred = arima_fit.forecast(steps=forecast_horizon)\n",
    "        arima_metrics = calculate_metrics(test_data, arima_pred)\n",
    "        arima_metrics_list.append(arima_metrics)\n",
    "        \n",
    "        # Prophet\n",
    "        prophet_df = train_data.reset_index()[['Date', 'Close', 'Diluted EPS', 'Free Cash Flow', 'Net Debt', 'EBITDA', 'Sentiment_Score']]\n",
    "        prophet_df = prophet_df.rename(columns={'Date': 'ds', 'Close': 'y'})\n",
    "        \n",
    "        prophet_model = Prophet(daily_seasonality=True, yearly_seasonality=True, weekly_seasonality=True,\n",
    "                               changepoint_prior_scale=0.05, mcmc_samples=0)\n",
    "        for regressor in ['Diluted EPS', 'Free Cash Flow', 'Net Debt', 'EBITDA', 'Sentiment_Score']:\n",
    "            prophet_model.add_regressor(regressor)\n",
    "        prophet_model.fit(prophet_df)\n",
    "        future = prophet_model.make_future_dataframe(periods=forecast_horizon)\n",
    "        for regressor in ['Diluted EPS', 'Free Cash Flow', 'Net Debt', 'EBITDA', 'Sentiment_Score']:\n",
    "            future[regressor] = prophet_df[regressor].ffill().iloc[-1]\n",
    "        prophet_pred_df = prophet_model.predict(future)\n",
    "        prophet_pred = prophet_pred_df['yhat'].tail(forecast_horizon)\n",
    "        prophet_metrics = calculate_metrics(test_data, prophet_pred)\n",
    "        prophet_metrics_list.append(prophet_metrics)\n",
    "    \n",
    "    # Robust averaging\n",
    "    avg_metrics = {\n",
    "        'ARIMA': {\n",
    "            'RMSE': trim_mean([m['RMSE'] for m in arima_metrics_list], proportiontocut=0.1) if arima_metrics_list else np.nan,\n",
    "            'MAE': trim_mean([m['MAE'] for m in arima_metrics_list], proportiontocut=0.1) if arima_metrics_list else np.nan,\n",
    "            'MAPE': trim_mean([m['MAPE'] for m in arima_metrics_list if not np.isnan(m['MAPE'])], proportiontocut=0.1) if any(not np.isnan(m['MAPE']) for m in arima_metrics_list) else np.nan\n",
    "        },\n",
    "        'Prophet': {\n",
    "            'RMSE': trim_mean([m['RMSE'] for m in prophet_metrics_list], proportiontocut=0.1) if prophet_metrics_list else np.nan,\n",
    "            'MAE': trim_mean([m['MAE'] for m in prophet_metrics_list], proportiontocut=0.1) if prophet_metrics_list else np.nan,\n",
    "            'MAPE': trim_mean([m['MAPE'] for m in prophet_metrics_list if not np.isnan(m['MAPE'])], proportiontocut=0.1) if any(not np.isnan(m['MAPE']) for m in prophet_metrics_list) else np.nan\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    logging.info(f\"Walk-Forward Validation Results: {avg_metrics}\")\n",
    "    return avg_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bfd6c1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_predictions(data, arima_forecast, prophet_forecast, prophet_forecast_df, backtest_arima_df, backtest_prophet_df, ticker, forecast_horizon=7):\n",
    "    \"\"\"\n",
    "    Plot predictions with dynamic titles and enhanced sentiment visualization.\n",
    "    Adjusted: Dynamic titles, significant sentiment highlighting, save plot.\n",
    "    \"\"\"\n",
    "    last_date = data.index[-1]\n",
    "    future_dates = [last_date + timedelta(days=i+1) for i in range(forecast_horizon)]\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 8), sharey=True)\n",
    "    \n",
    "    # ARIMA subplot\n",
    "    ax1.plot(data.index, data['Close'], label='Historical Close', color='blue')\n",
    "    if arima_forecast is not None:\n",
    "        ax1.plot(future_dates, arima_forecast, label='ARIMA Forecast', color='red', linestyle='--')\n",
    "    if backtest_arima_df is not None:\n",
    "        buy_signals = backtest_arima_df[backtest_arima_df['Signal'] == 1]\n",
    "        sell_signals = backtest_arima_df[backtest_arima_df['Signal'] == -1]\n",
    "        ax1.scatter(buy_signals['Date'], buy_signals['Close'], color='green', marker='^', label='ARIMA Buy Signal', alpha=0.7)\n",
    "        ax1.scatter(sell_signals['Date'], sell_signals['Close'], color='red', marker='v', label='ARIMA Sell Signal', alpha=0.7)\n",
    "    \n",
    "    # Sentiment on secondary axis\n",
    "    if 'Sentiment_Score' in data.columns:\n",
    "        ax1_sent = ax1.twinx()\n",
    "        sentiment_mask = (data['Sentiment_Score'] != 0.0) & (data['Sentiment_Score'].abs() > 0.3)\n",
    "        if sentiment_mask.any():\n",
    "            ax1_sent.plot(data.index[sentiment_mask], data['Sentiment_Score'][sentiment_mask], \n",
    "                         label='Significant Sentiment', color='purple', alpha=0.3)\n",
    "            ax1_sent.set_ylabel('Sentiment Score (-1 to 1)', color='purple')\n",
    "            ax1_sent.tick_params(axis='y', labelcolor='purple')\n",
    "            ax1_sent.legend(loc='upper right')\n",
    "    \n",
    "    ax1.set_title(f'ARIMA: {ticker} Stock Price Prediction')\n",
    "    ax1.set_xlabel('Date')\n",
    "    ax1.set_ylabel('Close Price (USD)')\n",
    "    ax1.legend(loc='upper left')\n",
    "    ax1.grid(True)\n",
    "    \n",
    "    # Prophet subplot\n",
    "    ax2.plot(data.index, data['Close'], label='Historical Close', color='blue')\n",
    "    if prophet_forecast is not None:\n",
    "        ax2.plot(future_dates, prophet_forecast, label='Prophet Forecast', color='green', linestyle='--')\n",
    "        ax2.fill_between(future_dates, \n",
    "                         prophet_forecast_df['yhat_lower'].tail(forecast_horizon), \n",
    "                         prophet_forecast_df['yhat_upper'].tail(forecast_horizon), \n",
    "                         color='green', alpha=0.1, label='Prophet Confidence Interval')\n",
    "    if backtest_prophet_df is not None:\n",
    "        buy_signals = backtest_prophet_df[backtest_prophet_df['Signal'] == 1]\n",
    "        sell_signals = backtest_prophet_df[backtest_prophet_df['Signal'] == -1]\n",
    "        ax2.scatter(buy_signals['Date'], buy_signals['Close'], color='lime', marker='^', label='Prophet Buy Signal', alpha=0.7)\n",
    "        ax2.scatter(sell_signals['Date'], sell_signals['Close'], color='darkred', marker='v', label='Prophet Sell Signal', alpha=0.7)\n",
    "    \n",
    "    # Sentiment on secondary axis\n",
    "    if 'Sentiment_Score' in data.columns:\n",
    "        ax2_sent = ax2.twinx()\n",
    "        if sentiment_mask.any():\n",
    "            ax2_sent.plot(data.index[sentiment_mask], data['Sentiment_Score'][sentiment_mask], \n",
    "                         label='Significant Sentiment', color='purple', alpha=0.3)\n",
    "            ax2_sent.set_ylabel('Sentiment Score (-1 to 1)', color='purple')\n",
    "            ax2_sent.tick_params(axis='y', labelcolor='purple')\n",
    "            ax2_sent.legend(loc='upper right')\n",
    "    \n",
    "    ax2.set_title(f'Prophet: {ticker} Stock Price Prediction')\n",
    "    ax2.set_xlabel('Date')\n",
    "    ax2.legend(loc='upper left')\n",
    "    ax2.grid(True)\n",
    "    \n",
    "    plt.suptitle(f'{ticker} Stock Price Prediction and Backtest Signals (7-Day Forecast)', y=1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{ticker}_stock_price_predictions.png', dpi=300)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd12266",
   "metadata": {},
   "source": [
    "**Main**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bd031d8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-05 19:03:55,256 - INFO - Set CMDSTAN directory to C:\\Users\\nguye/.cmdstan\n",
      "2025-05-05 19:03:55,261 - INFO - Processing ticker: AAPL\n",
      "2025-05-05 19:03:55,268 - ERROR - Error preparing data for AAPL: Index(['Ticker'], dtype='object'). Skipping ticker.\n",
      "2025-05-05 19:03:55,269 - ERROR - Skipping AAPL: Insufficient data (<100 days)\n"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "forecast_horizon = 7\n",
    "data_dir = Path('../data')\n",
    "stock_csv = data_dir / 'cleaned_stock_data.csv'\n",
    "\n",
    "# Configure cmdstanpy\n",
    "cmdstan_dir = os.path.expanduser('~/.cmdstan')\n",
    "os.makedirs(cmdstan_dir, exist_ok=True)\n",
    "os.environ['CMDSTAN'] = cmdstan_dir\n",
    "logging.info(f\"Set CMDSTAN directory to {cmdstan_dir}\")\n",
    "\n",
    "# Get tickers\n",
    "tickers = pd.read_csv(stock_csv)['Ticker'].unique()[:5]  # Process up to 5 tickers\n",
    "results = []\n",
    "\n",
    "for ticker in tickers:\n",
    "    logging.info(f\"Processing ticker: {ticker}\")\n",
    "    balance_csv = data_dir / f'balance_sheet_{ticker}.csv'\n",
    "    cashflow_csv = data_dir / f'cash_flow_{ticker}.csv'\n",
    "    income_csv = data_dir / f'income_statement_{ticker}.csv'\n",
    "    news_csv = data_dir / 'news_data.csv'\n",
    "    \n",
    "    # Load and prepare data\n",
    "    data, scaler = load_and_prepare_data(stock_csv, balance_csv, cashflow_csv, income_csv, ticker)\n",
    "    if data is None or len(data) < 100:\n",
    "        logging.error(f\"Skipping {ticker}: Insufficient data (<100 days)\")\n",
    "        continue\n",
    "    \n",
    "    # Load news data and merge\n",
    "    sentiment_df = load_news_data(news_csv, ticker, data.index)\n",
    "    if sentiment_df is not None:\n",
    "        data = data.reset_index().merge(sentiment_df[['Date', 'Sentiment_Score']], on='Date', how='left')\n",
    "        data['Sentiment_Score'] = data['Sentiment_Score'].fillna(0.0)\n",
    "        data = data.set_index('Date')\n",
    "        logging.info(f\"Added Sentiment_Score to data for {ticker}. Columns: {list(data.columns)}\")\n",
    "    else:\n",
    "        logging.warning(f\"No sentiment data for {ticker}. Using neutral scores.\")\n",
    "        data['Sentiment_Score'] = 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce1f68a4",
   "metadata": {},
   "source": [
    "**Models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3fe36325",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# ARIMA forecast\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m arima_forecast, arima_metrics, arima_order \u001b[38;5;241m=\u001b[39m arima_forecast(\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mClose\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m, forecast_horizon)\n",
      "\u001b[1;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "# ARIMA forecast\n",
    "arima_forecast, arima_metrics, arima_order = arima_forecast(data['Close'], forecast_horizon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "060ab632",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prophet forecast with tuning\n",
    "prophet_forecast, prophet_metrics, prophet_forecast_df, prophet_scale = tune_prophet(data, forecast_horizon)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f185a56",
   "metadata": {},
   "source": [
    "**Walk-forward validation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "927ee1fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_metrics = walk_forward_validation(data, forecast_horizon)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b30649a1",
   "metadata": {},
   "source": [
    "**Backtesting**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be3d208",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backtesting\n",
    "if len(data) >= 100:\n",
    "    # ARIMA historical predictions\n",
    "    arima_model = auto_arima(data['Close'][:-forecast_horizon], seasonal=False, max_p=7, max_q=7, max_d=2)\n",
    "    arima_fit = ARIMA(data['Close'][:-forecast_horizon], order=arima_model.order).fit()\n",
    "    arima_hist_pred = arima_fit.predict(start=0, end=len(data)-forecast_horizon-1)\n",
    "    arima_backtest_results, arima_backtest_df = backtest_strategy(data.iloc[:-forecast_horizon], arima_hist_pred, 'ARIMA')\n",
    "    \n",
    "    # Prophet historical predictions\n",
    "    train_data = data.iloc[:-forecast_horizon]\n",
    "    prophet_df = train_data.reset_index()[['Date', 'Close', 'Diluted EPS', 'Free Cash Flow', 'Net Debt', 'EBITDA', 'Sentiment_Score']]\n",
    "    prophet_df = prophet_df.drop_duplicates(subset='Date')\n",
    "    prophet_df = prophet_df.rename(columns={'Date': 'ds', 'Close': 'y'})\n",
    "    \n",
    "    prophet_model = Prophet(daily_seasonality=True, yearly_seasonality=True, weekly_seasonality=True,\n",
    "                            changepoint_prior_scale=prophet_scale, mcmc_samples=0)\n",
    "    for regressor in ['Diluted EPS', 'Free Cash Flow', 'Net Debt', 'EBITDA', 'Sentiment_Score']:\n",
    "        prophet_model.add_regressor(regressor)\n",
    "    prophet_model.fit(prophet_df)\n",
    "    \n",
    "    prophet_pred_df = prophet_model.predict(prophet_df)\n",
    "    prophet_pred_df = prophet_pred_df.set_index('ds').reindex(train_data.index, method='ffill')\n",
    "    prophet_hist_pred = prophet_pred_df['yhat']\n",
    "    \n",
    "    prophet_backtest_results, prophet_backtest_df = backtest_strategy(train_data, prophet_hist_pred, 'Prophet')\n",
    "else:\n",
    "    arima_backtest_results, arima_backtest_df = None, None\n",
    "    prophet_backtest_results, prophet_backtest_df = None, None\n",
    "    logging.warning(f\"Insufficient data for backtesting {ticker}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a44dd9",
   "metadata": {},
   "source": [
    "**Plot predictions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d27191",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot predictions\n",
    "plot_predictions(data, arima_forecast, prophet_forecast, prophet_forecast_df,\n",
    "                arima_backtest_df, prophet_backtest_df, ticker, forecast_horizon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "898de895",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save predictions\n",
    "if arima_forecast is not None and prophet_forecast is not None:\n",
    "    future_dates = [data.index[-1] + timedelta(days=i+1) for i in range(forecast_horizon)]\n",
    "    pred_df = pd.DataFrame({\n",
    "        'Date': future_dates,\n",
    "        'ARIMA_Prediction': arima_forecast,\n",
    "        'Prophet_Prediction': prophet_forecast\n",
    "    })\n",
    "    pred_df.to_csv(data_dir / f'{ticker}_stock_price_predictions.csv', index=False)\n",
    "    logging.info(f\"Saved predictions for {ticker}\")\n",
    "\n",
    "# Save metrics\n",
    "metrics_df = pd.DataFrame({\n",
    "    'Model': ['ARIMA', 'Prophet'],\n",
    "    'RMSE': [arima_metrics.get('RMSE', np.nan), prophet_metrics.get('RMSE', np.nan)],\n",
    "    'MAE': [arima_metrics.get('MAE', np.nan), prophet_metrics.get('MAE', np.nan)],\n",
    "    'MAPE': [arima_metrics.get('MAPE', np.nan), prophet_metrics.get('MAPE', np.nan)],\n",
    "    'Best Parameters': [f\"Order: {arima_order}\", f\"changepoint_prior_scale: {prophet_scale or 0.05}\"]\n",
    "})\n",
    "metrics_df.to_csv(data_dir / f'{ticker}_model_metrics.csv', index=False)\n",
    "logging.info(f\"Saved metrics for {ticker}\")\n",
    "\n",
    "# Save backtest results\n",
    "if arima_backtest_results and prophet_backtest_results:\n",
    "    backtest_df = pd.DataFrame({\n",
    "        'Model': ['ARIMA', 'Prophet'],\n",
    "        'Cumulative Return (%)': [arima_backtest_results['Cumulative Return (%)'], \n",
    "                                prophet_backtest_results['Cumulative Return (%)']],\n",
    "        'Number of Trades': [arima_backtest_results['Number of Trades'], \n",
    "                            prophet_backtest_results['Number of Trades']]\n",
    "    })\n",
    "    backtest_df.to_csv(data_dir / f'{ticker}_backtest_results.csv', index=False)\n",
    "    logging.info(f\"Saved backtest results for {ticker}\")\n",
    "\n",
    "# Select best model\n",
    "best_model = 'Prophet' if avg_metrics['Prophet']['RMSE'] < avg_metrics['ARIMA']['RMSE'] else 'ARIMA'\n",
    "results.append({'Ticker': ticker, 'Best_Model': best_model, 'Metrics': avg_metrics})\n",
    "\n",
    "# Save summary\n",
    "summary_df = pd.DataFrame(results)\n",
    "summary_df.to_csv(data_dir / 'summary_results.csv', index=False)\n",
    "logging.info(\"Saved summary results\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
